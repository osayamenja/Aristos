{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:12.139728Z",
     "start_time": "2024-05-07T04:01:12.136402Z"
    }
   },
   "source": [
    "from main import (grigora2, Worker, Group, expert_parallel_group_objective_function,\n",
    "                  all_reduce_function, gamma_function, Expert, expert_assignment)\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation of Lysi (renamed from Grigora)\n",
    "Let's perform realistic experiments using the sharding specification generated by Lysi.\n",
    "\n",
    "Specifically, we will use this [script](https://github.com/osayamenja/Megatron-DeepSpeed/blob/main/examples_deepspeed/MoE/ds_pretrain_gpt_350M_MoE128.sh) and train GPT-3 16x350M on four Perlmutter [GPU nodes](https://docs.nersc.gov/systems/perlmutter/architecture/#gpu-nodes) "
   ],
   "id": "1b98cda01d83063d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below, we define the number of workers and number of GPUs per node",
   "id": "f4b000c639d1d501"
  },
  {
   "cell_type": "code",
   "source": [
    "dim = 16\n",
    "intra_node_width = 4.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:14.824441Z",
     "start_time": "2024-05-07T04:01:14.821864Z"
    }
   },
   "id": "81336023e453319",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, we build the adjacency matrix. We manually obtained the alpha and beta values below via NCCL-tests mirco-benchmarks. \n",
    "We anticipate automating this network profiling procedure."
   ],
   "id": "e63a895718329b57"
  },
  {
   "cell_type": "code",
   "source": [
    "adjacency = np.zeros((dim, dim, 2))\n",
    "intra_node_cost = (0.009, 0.014)  # (ms, ms/MB)\n",
    "inter_node_cost = (0.03, 0.054)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:15.228065Z",
     "start_time": "2024-05-07T04:01:15.224385Z"
    }
   },
   "id": "98f56737dc97d3e7",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that each GPU connects to a separate NIC in the Perlmutter; thus, there are only two types of links: intra-node NVLink and internode NIC connections.",
   "id": "3f356c44a3d0296f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:15.599485Z",
     "start_time": "2024-05-07T04:01:15.594934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for ii in range(adjacency.shape[0]):\n",
    "    for jj in range(adjacency.shape[0]):\n",
    "        if ii != jj and math.floor(jj / intra_node_width) == math.floor(ii / intra_node_width):\n",
    "            # intra-node\n",
    "            adjacency[ii, jj] = intra_node_cost\n",
    "        else:\n",
    "            # inter-node\n",
    "            adjacency[ii, jj] = inter_node_cost"
   ],
   "id": "f503dad0a433dcf0",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Below outlines the theoretical FLOPS of the tensor core in the A100 GPU, which comprises our testbed. We used the values from official [documentation](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf), *but* we obtained the realistic scaling factor from empirical measurements. \n",
    "\n",
    "Note this scale is much less than the 75% reported by [NVIDIA](https://forums.developer.nvidia.com/t/about-gpu-peak-performance/264462/5) but aligns with the [literature](https://ieeexplore.ieee.org/document/9415606), which details 40% utilization for $4096\\times4096$ matrices on the V100."
   ],
   "id": "d2506dfef2796c46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The expert matrix GEMMs for GPT-3 MoE are described below. Note that $\\bigotimes$ denotes matrix multiplication, $s$ sequence length, $h$ hidden size, and $b$ batch size.\n",
    "$$(s\\cdot b,\\; h)\\bigotimes (h, \\;4h) = (2048\\cdot 4, \\;1024) \\bigotimes (1024, \\;4096)$$   "
   ],
   "id": "e1e95d9b44e718d5"
  },
  {
   "cell_type": "code",
   "source": [
    "a100_theoretical_flop_per_ms = 312 * 1E9\n",
    "realistic_scaling_factor = 0.43\n",
    "real_flops = int(math.ceil(realistic_scaling_factor * a100_theoretical_flop_per_ms))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:16.526450Z",
     "start_time": "2024-05-07T04:01:16.524206Z"
    }
   },
   "id": "5c24c65bbca492b0",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:16.942838Z",
     "start_time": "2024-05-07T04:01:16.940253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mem = 32\n",
    "workers = []\n",
    "for ii in range(adjacency.shape[0]):\n",
    "    workers.append(Worker(ii, real_flops, mem))"
   ],
   "id": "2b9ca12ab22947d1",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use the below `dict` for later expert allocation.",
   "id": "9e8ab37881aea70c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:17.760618Z",
     "start_time": "2024-05-07T04:01:17.757483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# needed for later\n",
    "workers_info: Dict[int, Worker] = dict()\n",
    "for i in range(len(workers)):\n",
    "    workers_info.update({i: workers[i]})"
   ],
   "id": "124f32de6ae7b01c",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We define the experts below.",
   "id": "2006a44b8fa2a3fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:18.483290Z",
     "start_time": "2024-05-07T04:01:18.480899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_exp = 64\n",
    "exp = []\n",
    "experts = []\n",
    "exp_flops = 16 * 4 * 2048 * (1024 ** 2)\n",
    "for ii in range(n_exp):\n",
    "    exp.append(exp_flops)\n",
    "    experts.append(Expert(exp_flops, ii))"
   ],
   "id": "d92890f9dc082d7b",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ensure to check this [file](grigora_manuscript.pdf) for more details.",
   "id": "e6d3c5600a4713f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:19.348749Z",
     "start_time": "2024-05-07T04:01:19.346487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p2p_buf_mb = 16\n",
    "p2p_fr = 4\n",
    "all_r_buf = 512\n",
    "\n",
    "gamma_arguments = {Group.NUM_LAYERS: 24,\n",
    "                   Group.GLOBAL_BATCH_SIZE: 256,\n",
    "                   Group.MINI_BATCH_SIZE: 4,\n",
    "                   Group.MOE_FREQUENCY: 2,\n",
    "                   Group.RECOMPUTATION_AMOUNT: 1}"
   ],
   "id": "994aa04c9acad76",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:24.168860Z",
     "start_time": "2024-05-07T04:01:24.160033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "shard_spec, inv = grigora2(a=adjacency,\n",
    "                               obj=expert_parallel_group_objective_function,\n",
    "                               all_reduce_func=all_reduce_function,\n",
    "                               gamma=gamma_function,\n",
    "                               p2p_buffer_size=p2p_buf_mb,\n",
    "                               p2p_freq=p2p_fr,\n",
    "                               all_reduce_buffer_size=all_r_buf,\n",
    "                               workers=workers,\n",
    "                               expert_workload=exp,\n",
    "                               gamma_args=gamma_arguments)\n",
    "print(shard_spec.subsets())"
   ],
   "id": "3220bc550b1f04d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As shown above, Lysi produces a sharding specification where ranks of workers are grouped into communication-optimal groups.\n",
    "Lysi computes that as defined concretely above, for 64 experts and 16 GPUs, all GPUs should be in a single expert parallel group."
   ],
   "id": "ac236a239d7194af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now lets optimally shard experts across workers given this parallelism strategy.",
   "id": "a77a190b78d8ca06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T04:01:27.986267Z",
     "start_time": "2024-05-07T04:01:27.980558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for group in shard_spec.subsets():\n",
    "    expert_parallel_workers: List[Worker] = []\n",
    "    for worker in group:\n",
    "        expert_parallel_workers.append(workers_info[worker])\n",
    "    print(expert_assignment(expert_parallel_workers, experts))"
   ],
   "id": "b318360f9cb8003",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Worker(id=0, flops=134160000000, mem_capacity=32): {0, 61, 62, 63}, Worker(id=1, flops=134160000000, mem_capacity=32): {1, 58, 59, 60}, Worker(id=2, flops=134160000000, mem_capacity=32): {56, 57, 2, 55}, Worker(id=3, flops=134160000000, mem_capacity=32): {3, 52, 53, 54}, Worker(id=4, flops=134160000000, mem_capacity=32): {49, 50, 51, 4}, Worker(id=5, flops=134160000000, mem_capacity=32): {48, 5, 46, 47}, Worker(id=6, flops=134160000000, mem_capacity=32): {43, 44, 45, 6}, Worker(id=7, flops=134160000000, mem_capacity=32): {40, 41, 42, 7}, Worker(id=8, flops=134160000000, mem_capacity=32): {8, 37, 38, 39}, Worker(id=9, flops=134160000000, mem_capacity=32): {9, 34, 35, 36}, Worker(id=10, flops=134160000000, mem_capacity=32): {32, 33, 10, 31}, Worker(id=11, flops=134160000000, mem_capacity=32): {11, 28, 29, 30}, Worker(id=12, flops=134160000000, mem_capacity=32): {25, 26, 27, 12}, Worker(id=13, flops=134160000000, mem_capacity=32): {24, 13, 22, 23}, Worker(id=14, flops=134160000000, mem_capacity=32): {19, 20, 21, 14}, Worker(id=15, flops=134160000000, mem_capacity=32): {16, 17, 18, 15}}\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As depicted above, due to the homogeneity of experts and workers, the best assignment is an equal number of experts, specifically $\\frac{64}{16} = 4$ per worker.\n",
    "\n",
    "This is a trivial case, especially considering the memory abundance per worker; a more challenging scenario arises when the experts and workers are heterogeneous. Our algorithm still finds an optimal assignment; we encourage you to experiment and verify this claim for yourself. "
   ],
   "id": "fb576b75d10c92c7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
