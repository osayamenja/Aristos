{
  "capacity_factor": 1,
  "drop_tokens": 0,
  "expert_top_k": 1,
  "global_batch": 256,
  "is_training": 0,
  "hidden_act": 0,
  "hidden_size": 1024,
  "intermediate_size": 4096,
  "mini_batch": 4,
  "moe_frequency": 1,
  "num_experts": 1,
  "num_layers": 20,
  "sequence_len": 2048,
  "torch_dtype": 0,
  "vocab_size": 32000
}