{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "Schema for Kleos configuration",
  "type": "object",
  "properties": {
    "capacity_factor": {
      "type": "integer",
      "description": "Capacity factor for expert",
      "minimum": 1
    },
    "drop_tokens": {
      "enum": [0, 1],
      "description": "Indicates whether to drop tokens when capacity is exceeded"
    },
    "expert_top_k": {
      "type": "integer",
      "description": "Top k selected experts",
      "minimum": 1
    },
    "global_batch": {
      "type": "integer",
      "description": "Global input batch",
      "multipleOf": 2
    },
    "is_training": {
      "enum": [0, 1],
      "description": "Indicates whether this a training ot inference job"
    },
    "hidden_act": {
      "enum": [0, 1],
      "$comment": "0: relu, 1: gelu",
      "description": "Activation function. We use integers for compatibility with C++ templates"
    },
    "hidden_size": {
      "type": "integer",
      "description": "Embedding dimension",
      "multipleOf": 64
    },
    "intermediate_size": {
      "type": "integer",
      "description": "Intermediate dimension for the FFN/expert",
      "multipleOf": 64
    },
    "mini_batch": {
      "type": "integer",
      "description": "Batch of token sequences per step",
      "multipleOf": 2
    },
    "moe_frequency": {
      "type": "integer",
      "description": "MoE frequency",
      "minimum": 1
    },
    "num_experts": {
      "type": "integer",
      "description": "Total number of experts",
      "minimum": 1
    },
    "sequence_len": {
      "type": "integer",
      "description": "Sequence length for either training or inference",
      "multipleOf": 128
    },
    "torch_dtype": {
      "enum": [0, 1, 2, 3],
      "$comment": "0: float, 1: tf32, 2: bf16, 3: fp16",
      "description": "Data type"
    },
    "vocab_size": {
      "type": "integer",
      "description": "Vocabulary size"
    }
  },
  "required": [
    "capacity_factor",
    "drop_tokens",
    "expert_top_k",
    "is_training",
    "hidden_act",
    "hidden_size",
    "intermediate_size",
    "mini_batch",
    "moe_frequency",
    "num_experts",
    "num_layers",
    "sequence_len",
    "torch_dtype",
    "vocab_size"
  ]
}